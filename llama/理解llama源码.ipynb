{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 理解 llama 源码\n",
    "本例从零开始基于transformers库逐模块搭建和解读Llama模型源码。\n",
    "\n",
    "并且训练它来实现一个有趣的实例：两数之和。\n",
    "\n",
    "输入输出类似如下：\n",
    "\n",
    "输入：\"12345+54321=\"\n",
    "\n",
    "输出：\"66666\"\n",
    "\n",
    "我们把这个任务当做一个文本生成任务来进行。输入是一个序列的上半部分，输出其下半部分.\n",
    "\n",
    "这和文本生成的输入输出结构是类似的，所以可以用Llama来做。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad2f6591-d942-4371-9f10-0ff02c0f1d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "# 定义字典\n",
    "words = '<PAD>,<BOS>,<EOS>,1,2,3,4,5,6,7,8,9,0,+,='\n",
    "vocab = {word: i for i, word in enumerate(words.split(','))}\n",
    "vocab_r = [k for k, v in vocab.items()] #反查词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49560847-ba4e-4626-ab41-a06c80dd1151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS>457975526213193+417530444185=458393056657378<EOS> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#两数相加数据集\n",
    "def get_data(min_length=10,max_length=20):\n",
    "    # 定义词集合\n",
    "    words = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "    # 每个词被选中的概率\n",
    "    p = np.array([7, 5, 5, 7, 6, 5, 7, 6, 5, 7])\n",
    "    p = p / p.sum()\n",
    "\n",
    "    # 随机采样n1个词作为s1\n",
    "    n1 = random.randint(min_length, max_length)\n",
    "    s1 = np.random.choice(words, size=n1, replace=True, p=p)\n",
    "    s1 = s1.tolist()\n",
    "\n",
    "    # 随机采样n2个词作为s2\n",
    "    n2 = random.randint(min_length, max_length)\n",
    "    s2 = np.random.choice(words, size=n2, replace=True, p=p)\n",
    "    s2 = s2.tolist()\n",
    "\n",
    "    # x等于s1和s2字符上的相加\n",
    "    x = s1 + ['+'] + s2 + ['=']\n",
    "    \n",
    "    # y等于s1和s2数值上的相加\n",
    "    y = int(''.join(s1)) + int(''.join(s2))\n",
    "    y = list(str(y))\n",
    "    \n",
    "    # 加上首尾符号\n",
    "    x = ['<BOS>'] + x \n",
    "    y =  y + ['<EOS>']\n",
    "    \n",
    "    return x,y\n",
    "\n",
    "x,y = get_data() \n",
    "print(''.join(x)+''.join(y),\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10ddc6c8-83ad-45a9-a4cb-6a0e00898ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS>415831276028781+0863167459071071=1278998735099852<EOS>\n"
     ]
    }
   ],
   "source": [
    "# 定义数据集\n",
    "class TwoSumDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,size = 100000, min_length=10,max_length=20):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.size = size\n",
    "        self.min_length=min_length\n",
    "        self.max_length=max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x,y = self.get(i)\n",
    "        \n",
    "        # 编码成token\n",
    "        context_ids = [vocab[i] for i in x]\n",
    "        target_ids = [vocab[i] for i in y]\n",
    "        \n",
    "        input_ids = context_ids + target_ids\n",
    "        \n",
    "        #-100标志位后面会在计算loss时会被忽略不贡献损失，我们集中优化target部分生成的loss\n",
    "        labels = [-100]*len(context_ids)+ target_ids\n",
    "        masks = [0 if t==vocab['<PAD>'] else 1 for t in input_ids]\n",
    "        \n",
    "        example = {'input_ids':input_ids,\n",
    "                   'labels':labels,\n",
    "                   'attention_mask':masks}\n",
    "        \n",
    "        return example\n",
    "    \n",
    "    def get(self,i):\n",
    "        return get_data(self.min_length,self.max_length)\n",
    "    \n",
    "    \n",
    "    def show_example(self,example):\n",
    "        input_ids,labels = example['input_ids'],example['labels']\n",
    "        x = ''.join([vocab_r[a] for a,b in zip(input_ids,labels) if b==-100])\n",
    "        y = ''.join([vocab_r[a] for a,b in zip(input_ids,labels) if b!=-100])\n",
    "        print(x+y)\n",
    "        \n",
    "        \n",
    "    \n",
    "ds_train = TwoSumDataset(size = 100000,min_length=10,max_length=20)\n",
    "ds_val = TwoSumDataset(size = 10000,min_length=10,max_length=20)\n",
    "example = ds_train[0]\n",
    "ds_train.show_example(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c91317c-921b-49b8-ba9b-35cfd0e20446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(examples: list):\n",
    "    len_ids = [len(example[\"input_ids\"]) for example in examples]\n",
    "    longest = max(len_ids) #之后按照batch中最长的input_ids进行padding\n",
    "    \n",
    "    input_ids = []\n",
    "    labels_list = []\n",
    "    masks_list = []\n",
    "    \n",
    "    for length, example in sorted(zip(len_ids, examples), key=lambda x: -x[0]):\n",
    "        ids = example[\"input_ids\"]\n",
    "        labs = example[\"labels\"]\n",
    "        masks = example['attention_mask']\n",
    "        \n",
    "        ids = [vocab['<PAD>']] * (longest - length)+ids \n",
    "        labs = [-100] * (longest - length)+labs\n",
    "        masks = [0]*(longest - length)+masks\n",
    "        \n",
    "        input_ids.append(torch.LongTensor(ids))\n",
    "        labels_list.append(torch.LongTensor(labs))\n",
    "        masks_list.append(torch.LongTensor(masks))\n",
    "          \n",
    "    input_ids = torch.stack(input_ids)\n",
    "    labels = torch.stack(labels_list)\n",
    "    attention_mask = torch.stack(masks_list)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "        \"attention_mask\":attention_mask\n",
    "    }\n",
    "\n",
    "# 数据加载器\n",
    "dl_train = DataLoader(dataset=ds_train,\n",
    "         batch_size=40,\n",
    "         drop_last=True,\n",
    "         shuffle=True,\n",
    "         collate_fn = data_collator        \n",
    "        )\n",
    "\n",
    "dl_val = DataLoader(dataset=ds_val,\n",
    "         batch_size=20,\n",
    "         drop_last=True,\n",
    "         shuffle=False,\n",
    "         collate_fn = data_collator  \n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4472db7c-5c5c-429a-90e2-84c1b1fc5c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dl_train:\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4df80e7-6f88-4526-9a9f-f8486f9afb94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 1, 11, 11,  ...,  3,  7,  2],\n",
       "         [ 0,  1, 10,  ..., 12,  8,  2],\n",
       "         [ 0,  0,  0,  ..., 12,  4,  2],\n",
       "         ...,\n",
       "         [ 0,  0,  0,  ...,  7, 11,  2],\n",
       "         [ 0,  0,  0,  ...,  3,  6,  2],\n",
       "         [ 0,  0,  0,  ..., 12, 11,  2]]),\n",
       " 'labels': tensor([[-100, -100, -100,  ...,    3,    7,    2],\n",
       "         [-100, -100, -100,  ...,   12,    8,    2],\n",
       "         [-100, -100, -100,  ...,   12,    4,    2],\n",
       "         ...,\n",
       "         [-100, -100, -100,  ...,    7,   11,    2],\n",
       "         [-100, -100, -100,  ...,    3,    6,    2],\n",
       "         [-100, -100, -100,  ...,   12,   11,    2]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [0, 1, 1,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beaaa9b4-cf4a-4ee1-84ff-5638d62170f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"attention_bias\": false,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 512,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 2752,\n",
       "  \"max_position_embeddings\": 128,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_hidden_layers\": 8,\n",
       "  \"num_key_value_heads\": 16,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.34.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 15\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n",
    "\n",
    "from transformers.models.llama.configuration_llama  import LlamaConfig\n",
    "from transformers.models.llama.modeling_llama import LLAMA_INPUTS_DOCSTRING,LLAMA_START_DOCSTRING\n",
    "\n",
    "logger = logging.get_logger('llama')\n",
    "\n",
    "config = LlamaConfig(\n",
    "    vocab_size=len(vocab),\n",
    "    hidden_size=512,\n",
    "    intermediate_size=2752,\n",
    "    num_hidden_layers=8,\n",
    "    num_attention_heads=16,\n",
    "    hidden_act='silu',\n",
    "    max_position_embeddings=128,\n",
    "    initializer_range=0.02,\n",
    "    rms_norm_eps=1e-06,\n",
    "    use_cache=True,\n",
    "    pad_token_id=0,\n",
    "    bos_token_id=1,\n",
    "    eos_token_id=2,\n",
    "    tie_word_embeddings=False\n",
    ") \n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c49d796-7a43-47f2-bb47-b89be9233443",
   "metadata": {},
   "source": [
    "我们观察RoPE位置编码第行的元素计算公式  $\\cos(k \\theta_{i}) = \\cos(k 10000^{-2i/d}) ，可以发现越大，三角函数对应的角频率系数越小，或者说越低频，对应的三角函数变化越慢。\n",
    "\n",
    "容易得到如下直观结论：短距离之间的差异(例如1和5的差异)，主要体现在高频分量(i比较小)上，长距离之间的差异(例如5000和10000的差异)，主要体现在低频分量(i比较大)上。\n",
    "\n",
    "为了在短距离情况下具有外推特性，而在长距离情况下具有内插特性，我们可以设计一个和有关的位置序号缩放因子，使得在最高频()时取值为1(与扩展前基本一致)，而在最低频时()恰好为缩放倍数的倒数(缩放到扩展前的范围)。\n",
    "\n",
    "一种有效的选择方案是的指数函数，其效果相当于对中的做一个缩放，根据边界条件容易求得合适的缩放因子为  。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2fd696e-a0ac-4512-90a9-914bc6045229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaRotaryEmbedding(torch.nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False) #persistent=False将不会作为state_dict\n",
    "\n",
    "        # Build here to make `torch.jit.trace` work.\n",
    "        self._set_cos_sin_cache(\n",
    "            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n",
    "        )\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n",
    "\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False)\n",
    "\n",
    "    def forward(self, x, seq_len=None):\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        #超过预设的max_position_embeddings则重新计算更大的Rope缓存，否则直接在缓存上切片\n",
    "        if seq_len > self.max_seq_len_cached: \n",
    "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        return (\n",
    "            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
    "            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
    "        )\n",
    "\n",
    "    \n",
    "class LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):\n",
    "    \"\"\"LlamaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n",
    "\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n",
    "        self.scaling_factor = scaling_factor\n",
    "        super().__init__(dim, max_position_embeddings, base, device)\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n",
    "        t = t / self.scaling_factor #线性内插相当于将位置序号等比例缩小\n",
    "\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False)\n",
    "\n",
    "\n",
    "class LlamaDynamicNTKScalingRotaryEmbedding(LlamaRotaryEmbedding):\n",
    "    \"\"\"LlamaRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n",
    "\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n",
    "        self.scaling_factor = scaling_factor\n",
    "        super().__init__(dim, max_position_embeddings, base, device)\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "\n",
    "        if seq_len > self.max_position_embeddings:\n",
    "            base = self.base * (\n",
    "                (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n",
    "            ) ** (self.dim / (self.dim - 2))  #NTK扩展方式直接对base进行缩放\n",
    "            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n",
    "            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n",
    "\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        \n",
    "        #此处处理逻辑与原始的ROPE有差异，原始逻辑如下\n",
    "        #emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        #emb[...,0::2]=freqs\n",
    "        #emb[...,1::2]=freqs\n",
    "        \n",
    "        \n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False)\n",
    "        \n",
    "        \n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    \n",
    "    #此处逻辑与原始的ROPE有所差异，原始逻辑如下\n",
    "    #x1 = x[..., 0::2] \n",
    "    #x2 = x[..., 1::2]\n",
    "    #res = torch.cat((x1, x2), dim=-1)\n",
    "    #res[...,0::2]=-x2\n",
    "    #res[...,1::2]=x1\n",
    "    #return res\n",
    "    \n",
    "    x1 = x[..., : x.shape[-1] // 2] \n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n",
    "    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.\n",
    "    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
    "    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "473b0067-fd18-4c8e-ba06-55c8731dd302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4, 8])\n",
      "tensor([[[[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000],\n",
      "          [ 0.5403,  0.9950,  0.9999,  1.0000,  0.5403,  0.9950,  0.9999,\n",
      "            1.0000],\n",
      "          [-0.4161,  0.9801,  0.9998,  1.0000, -0.4161,  0.9801,  0.9998,\n",
      "            1.0000],\n",
      "          [-0.9900,  0.9553,  0.9996,  1.0000, -0.9900,  0.9553,  0.9996,\n",
      "            1.0000]]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1,8,4,2)\n",
    "rope = LlamaRotaryEmbedding(dim=8)\n",
    "cos,sin = rope.forward(x,seq_len=4)\n",
    "print(cos.shape) \n",
    "print(cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "098330c2-dfc7-44c2-b1ea-83b49c977e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "\n",
    "class LlamaAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "\n",
    "        if (self.head_dim * self.num_heads) != self.hidden_size:\n",
    "            raise ValueError(\n",
    "                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n",
    "                f\" and `num_heads`: {self.num_heads}).\"\n",
    "            )\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n",
    "        self._init_rope()\n",
    "\n",
    "    def _init_rope(self):\n",
    "        if self.config.rope_scaling is None:\n",
    "            self.rotary_emb = LlamaRotaryEmbedding(self.head_dim, max_position_embeddings=self.max_position_embeddings)\n",
    "        else:\n",
    "            scaling_type = self.config.rope_scaling[\"type\"]\n",
    "            scaling_factor = self.config.rope_scaling[\"factor\"]\n",
    "            if scaling_type == \"linear\":\n",
    "                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(\n",
    "                    self.head_dim, max_position_embeddings=self.max_position_embeddings, scaling_factor=scaling_factor\n",
    "                )\n",
    "            elif scaling_type == \"dynamic\":\n",
    "                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(\n",
    "                    self.head_dim, max_position_embeddings=self.max_position_embeddings, scaling_factor=scaling_factor\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
    "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n",
    "            query_slices = self.q_proj.weight.split(\n",
    "                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n",
    "            )\n",
    "            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n",
    "            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n",
    "\n",
    "            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            query_states = torch.cat(query_states, dim=-1)\n",
    "\n",
    "            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            key_states = torch.cat(key_states, dim=-1)\n",
    "\n",
    "            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            value_states = torch.cat(value_states, dim=-1)\n",
    "\n",
    "        else:\n",
    "            query_states = self.q_proj(hidden_states)\n",
    "            key_states = self.k_proj(hidden_states)\n",
    "            value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        kv_seq_len = key_states.shape[-2]\n",
    "        if past_key_value is not None:\n",
    "            kv_seq_len += past_key_value[0].shape[-2]\n",
    "        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            # reuse k, v, self_attention\n",
    "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
    "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
    "\n",
    "        past_key_value = (key_states, value_states) if use_cache else None\n",
    "\n",
    "        # repeat k/v heads if n_kv_heads < n_heads\n",
    "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "\n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = attn_weights + attention_mask\n",
    "\n",
    "        # upcast attention to fp32\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
    "\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n",
    "            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n",
    "            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n",
    "        else:\n",
    "            attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        if not output_attentions:\n",
    "            attn_weights = None\n",
    "\n",
    "        return attn_output, attn_weights, past_key_value\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d115b81-0360-4252-98a2-40bfae72c0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
    "        self.act_fn = ACT2FN[config.hidden_act]\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            slice = self.intermediate_size // self.config.pretraining_tp\n",
    "            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)\n",
    "            up_proj_slices = self.up_proj.weight.split(slice, dim=0)\n",
    "            down_proj_slices = self.down_proj.weight.split(slice, dim=1)\n",
    "\n",
    "            gate_proj = torch.cat(\n",
    "                [F.linear(x, gate_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1\n",
    "            )\n",
    "            up_proj = torch.cat([F.linear(x, up_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1)\n",
    "\n",
    "            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)\n",
    "            down_proj = [\n",
    "                F.linear(intermediate_states[i], down_proj_slices[i]) for i in range(self.config.pretraining_tp)\n",
    "            ]\n",
    "            down_proj = sum(down_proj)\n",
    "        else:\n",
    "            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "        return down_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0564ab0-1bdb-4b59-9fb6-81d06d37ca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        LlamaRMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3087ee3c-0cee-4d5e-954d-d01759551b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.self_attn = LlamaAttention(config=config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        use_cache: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            use_cache (`bool`, *optional*):\n",
    "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
    "                (see `past_key_values`).\n",
    "            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n",
    "        \"\"\"\n",
    "\n",
    "        residual = hidden_states\n",
    "\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        # Self Attention\n",
    "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "            use_cache=use_cache,\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (self_attn_weights,)\n",
    "\n",
    "        if use_cache:\n",
    "            outputs += (present_key_value,)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71a00055-7fdb-43c7-9538-e06c0be00eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from transformers.models.bart.modeling_bart._make_causal_mask\n",
    "def _make_causal_mask(\n",
    "    input_ids_shape: torch.Size, dtype: torch.dtype, \n",
    "    device: torch.device, past_key_values_length: int = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    Make causal mask used for bi-directional self-attention.\n",
    "    \"\"\"\n",
    "    bsz, tgt_len = input_ids_shape\n",
    "    mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=device)\n",
    "    mask_cond = torch.arange(mask.size(-1), device=device)\n",
    "    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
    "    mask = mask.to(dtype)\n",
    "\n",
    "    if past_key_values_length > 0:\n",
    "        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype, device=device), mask], dim=-1)\n",
    "    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n",
    "\n",
    "\n",
    "# Copied from transformers.models.bart.modeling_bart._expand_mask\n",
    "def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n",
    "    \"\"\"\n",
    "    bsz, src_len = mask.size()\n",
    "    tgt_len = tgt_len if tgt_len is not None else src_len\n",
    "\n",
    "    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
    "    inverted_mask = 1.0 - expanded_mask\n",
    "\n",
    "    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n",
    "    LLAMA_START_DOCSTRING,\n",
    ")\n",
    "class LlamaPreTrainedModel(PreTrainedModel):\n",
    "    config_class = LlamaConfig\n",
    "    base_model_prefix = \"model\"\n",
    "    supports_gradient_checkpointing = True\n",
    "    _no_split_modules = [\"LlamaDecoderLayer\"]\n",
    "    _skip_keys_device_placement = \"past_key_values\"\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = self.config.initializer_range\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "\n",
    "    def _set_gradient_checkpointing(self, module, value=False):\n",
    "        if isinstance(module, LlamaModel):\n",
    "            module.gradient_checkpointing = value\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n",
    "    LLAMA_START_DOCSTRING,\n",
    ")\n",
    "class LlamaModel(LlamaPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n",
    "\n",
    "    Args:\n",
    "        config: LlamaConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super().__init__(config)\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embed_tokens = value\n",
    "\n",
    "    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask\n",
    "    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n",
    "        # create causal mask\n",
    "        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "        combined_attention_mask = None\n",
    "        if input_shape[-1] > 1:\n",
    "            combined_attention_mask = _make_causal_mask(\n",
    "                input_shape,\n",
    "                inputs_embeds.dtype,\n",
    "                device=inputs_embeds.device,\n",
    "                past_key_values_length=past_key_values_length,\n",
    "            )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]).to(\n",
    "                inputs_embeds.device\n",
    "            )\n",
    "            combined_attention_mask = (\n",
    "                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n",
    "            )\n",
    "\n",
    "        return combined_attention_mask\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # retrieve input_ids and inputs_embeds\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            batch_size, seq_length = input_ids.shape\n",
    "        elif inputs_embeds is not None:\n",
    "            batch_size, seq_length, _ = inputs_embeds.shape\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n",
    "\n",
    "        seq_length_with_past = seq_length\n",
    "        past_key_values_length = 0\n",
    "\n",
    "        if past_key_values is not None:\n",
    "            past_key_values_length = past_key_values[0][0].shape[2]\n",
    "            seq_length_with_past = seq_length_with_past + past_key_values_length\n",
    "\n",
    "        if position_ids is None:\n",
    "            device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "            position_ids = torch.arange(\n",
    "                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device\n",
    "            )\n",
    "            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n",
    "        else:\n",
    "            position_ids = position_ids.view(-1, seq_length).long()\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "        # embed positions\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(\n",
    "                (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device\n",
    "            )\n",
    "        attention_mask = self._prepare_decoder_attention_mask(\n",
    "            attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\n",
    "        )\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        if self.gradient_checkpointing and self.training:\n",
    "            if use_cache:\n",
    "                logger.warning_once(\n",
    "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                )\n",
    "                use_cache = False\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "\n",
    "        for idx, decoder_layer in enumerate(self.layers):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "\n",
    "            past_key_value = past_key_values[idx] if past_key_values is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        # None for past_key_value\n",
    "                        return module(*inputs, output_attentions, None)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(decoder_layer),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    position_ids,\n",
    "                    None,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = decoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask=attention_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    past_key_value=past_key_value,\n",
    "                    output_attentions=output_attentions,\n",
    "                    use_cache=use_cache,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = next_decoder_cache if use_cache else None\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
    "        return BaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attns,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d26854c9-a59c-4433-9f4b-52cdfc7fd1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_CONFIG_FOR_DOC = \"LlamaConfig\"\n",
    "\n",
    "class LlamaForCausalLM(LlamaPreTrainedModel):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = LlamaModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.model.embed_tokens = value\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def set_decoder(self, decoder):\n",
    "        self.model = decoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.model\n",
    "\n",
    "    # @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n",
    "    # @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n",
    "    ):\n",
    "        if past_key_values:\n",
    "            input_ids = input_ids[:, -1:]\n",
    "\n",
    "        position_ids = kwargs.get(\"position_ids\", None)\n",
    "        if attention_mask is not None and position_ids is None:\n",
    "            # create position_ids on the fly for batch generation\n",
    "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "            if past_key_values:\n",
    "                position_ids = position_ids[:, -1].unsqueeze(-1)\n",
    "\n",
    "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
    "        if inputs_embeds is not None and past_key_values is None:\n",
    "            model_inputs = {\"inputs_embeds\": inputs_embeds}\n",
    "        else:\n",
    "            model_inputs = {\"input_ids\": input_ids}\n",
    "\n",
    "        model_inputs.update(\n",
    "            {\n",
    "                \"position_ids\": position_ids,\n",
    "                \"past_key_values\": past_key_values,\n",
    "                \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "                \"attention_mask\": attention_mask,\n",
    "            }\n",
    "        )\n",
    "        return model_inputs\n",
    "\n",
    "    @staticmethod\n",
    "    def _reorder_cache(past_key_values, beam_idx):\n",
    "        reordered_past = ()\n",
    "        for layer_past in past_key_values:\n",
    "            reordered_past += (\n",
    "                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n",
    "            )\n",
    "        return reordered_past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5b382b7-7892-4ac3-afe7-c3f692715a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_start_docstrings(\n",
    "    \"\"\"\n",
    "    The LLaMa Model transformer with a sequence classification head on top (linear layer).\n",
    "\n",
    "    [`LlamaForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n",
    "    (e.g. GPT-2) do.\n",
    "\n",
    "    Since it does classification on the last token, it requires to know the position of the last token. If a\n",
    "    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n",
    "    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n",
    "    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n",
    "    each row of the batch).\n",
    "    \"\"\",\n",
    "    LLAMA_START_DOCSTRING,\n",
    ")\n",
    "class LlamaForSequenceClassification(LlamaPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.model = LlamaModel(config)\n",
    "        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.model.embed_tokens = value\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        transformer_outputs = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        logits = self.score(hidden_states)\n",
    "\n",
    "        if input_ids is not None:\n",
    "            batch_size = input_ids.shape[0]\n",
    "        else:\n",
    "            batch_size = inputs_embeds.shape[0]\n",
    "\n",
    "        if self.config.pad_token_id is None and batch_size != 1:\n",
    "            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n",
    "        if self.config.pad_token_id is None:\n",
    "            sequence_lengths = -1\n",
    "        else:\n",
    "            if input_ids is not None:\n",
    "                sequence_lengths = (torch.eq(input_ids, self.config.pad_token_id).long().argmax(-1) - 1).to(\n",
    "                    logits.device\n",
    "                )\n",
    "            else:\n",
    "                sequence_lengths = -1\n",
    "\n",
    "        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.to(logits.device)\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(pooled_logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(pooled_logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (pooled_logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=pooled_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e7063d3-9855-4f67-9fda-094f9490b611",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LlamaConfig(\n",
    "    vocab_size=len(vocab),\n",
    "    hidden_size=512,\n",
    "    intermediate_size=2752,\n",
    "    num_hidden_layers=8,\n",
    "    num_attention_heads=16,\n",
    "    num_key_value_heads=4,\n",
    "    rope_scaling = None,\n",
    "    hidden_act='silu',\n",
    "    max_position_embeddings=128,\n",
    "    initializer_range=0.02,\n",
    "    rms_norm_eps=1e-06,\n",
    "    use_cache=True,\n",
    "    pad_token_id=0,\n",
    "    bos_token_id=1,\n",
    "    eos_token_id=2,\n",
    "    tie_word_embeddings=False,\n",
    "    pretraining_tp = 1,\n",
    "    max_new_tokens = 100\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d41ee0de-7281-4e2a-89bf-84256792db76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.7917, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#试算一下\n",
    "model = LlamaForCausalLM(config)\n",
    "out = model.forward(**batch)\n",
    "print(out.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#! pip install torchkeras"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6e8e167-8834-402a-ba13-e70800c5fc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-13 11:48:53.910911: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-13 11:48:53.958498: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-13 11:48:53.958529: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-13 11:48:53.958550: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-13 11:48:53.967537: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-13 11:48:54.902692: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from torchkeras import KerasModel \n",
    "from accelerate import Accelerator \n",
    "\n",
    "class StepRunner:\n",
    "    def __init__(self, net, loss_fn, accelerator=None, stage = \"train\", metrics_dict = None, \n",
    "                 optimizer = None, lr_scheduler = None\n",
    "                 ):\n",
    "        self.net,self.loss_fn,self.metrics_dict,self.stage = net,loss_fn,metrics_dict,stage\n",
    "        self.optimizer,self.lr_scheduler = optimizer,lr_scheduler\n",
    "        self.accelerator = accelerator if accelerator is not None else Accelerator() \n",
    "        if self.stage=='train':\n",
    "            self.net.train() \n",
    "        else:\n",
    "            self.net.eval()\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \n",
    "        #loss\n",
    "        with self.accelerator.autocast():\n",
    "            loss = self.net(**batch).loss\n",
    "\n",
    "        #backward()\n",
    "        if self.stage==\"train\" and self.optimizer is not None:        \n",
    "            self.accelerator.backward(loss)\n",
    "            if self.accelerator.sync_gradients:\n",
    "                self.accelerator.clip_grad_norm_(self.net.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "        all_loss = self.accelerator.gather(loss).sum()\n",
    "        \n",
    "        #losses (or plain metrics that can be averaged)\n",
    "        step_losses = {self.stage+\"_loss\":all_loss.item()}\n",
    "        \n",
    "        #metrics (stateful metrics)\n",
    "        step_metrics = {}\n",
    "        \n",
    "        if self.stage==\"train\":\n",
    "            if self.optimizer is not None:\n",
    "                step_metrics['lr'] = self.optimizer.state_dict()['param_groups'][0]['lr']\n",
    "            else:\n",
    "                step_metrics['lr'] = 0.0\n",
    "        return step_losses,step_metrics\n",
    "    \n",
    "KerasModel.StepRunner = StepRunner \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21539355-3e32-4068-8614-4f52de0da996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[0;31m<<<<<< ⚡️ cuda is used >>>>>>\u001B[0m\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* background: */\n",
       "    progress::-webkit-progress-bar {background-color: #CDCDCD; width: 100%;}\n",
       "    progress {background-color: #CDCDCD;}\n",
       "\n",
       "    /* value: */\n",
       "    progress::-webkit-progress-value {background-color: #00BFFF  !important;}\n",
       "    progress::-moz-progress-bar {background-color: #00BFFF  !important;}\n",
       "    progress {color: #00BFFF ;}\n",
       "\n",
       "    /* optional */\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #000000;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='11' class='progress-bar-interrupted' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      91.67% [11/12] [2:36:34<14:14]\n",
       "      <br>\n",
       "      ████████████████████100.00% [500/500] [val_loss=0.0250]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-13 12:03:09,592] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001B[0;31m<<<<<< val_loss without improvement in 2 epoch,early stopping >>>>>> \n",
      "\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>lr</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.875147</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>1.495822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.488582</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.132379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.088114</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.054664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.050620</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.046760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.035627</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.032692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.028537</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.030355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.023164</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.029024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.020668</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.016860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.018016</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.012021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.016585</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.012539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.014194</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.025014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  train_loss       lr  val_loss\n",
       "0       1    1.875147  0.00003  1.495822\n",
       "1       2    0.488582  0.00003  0.132379\n",
       "2       3    0.088114  0.00003  0.054664\n",
       "3       4    0.050620  0.00003  0.046760\n",
       "4       5    0.035627  0.00003  0.032692\n",
       "5       6    0.028537  0.00003  0.030355\n",
       "6       7    0.023164  0.00003  0.029024\n",
       "7       8    0.020668  0.00003  0.016860\n",
       "8       9    0.018016  0.00003  0.012021\n",
       "9      10    0.016585  0.00003  0.012539\n",
       "10     11    0.014194  0.00003  0.025014"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_model = KerasModel(model,loss_fn = None,\n",
    "        optimizer=torch.optim.AdamW(model.parameters(),lr=3e-5))\n",
    "\n",
    "\n",
    "#加载 之前训练过的权重\n",
    "ckpt_path = 'llama_twosum'\n",
    "\n",
    "keras_model.fit(train_data = dl_train,\n",
    "                val_data = dl_val,\n",
    "                epochs=12,patience=2,\n",
    "                monitor='val_loss',mode='min',\n",
    "                ckpt_path = ckpt_path,\n",
    "                mixed_precision='fp16'\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64551e9a-f242-43a2-9a59-5defb560a5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation.utils import GenerationConfig\n",
    "model.generation_config = GenerationConfig.from_dict({'num_beams':1,\n",
    "                            'max_new_tokens':100,\n",
    "                            'max_length':200})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec53ad3a-934d-4930-bba2-1d6a171ade09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.num_beams=1\n",
    "model.generation_config.max_new_tokens = 100 \n",
    "model.generation_config.max_length=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67462a79-bec0-4c33-a57a-f4d65d551bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ans(tensor) ->\"str\":\n",
    "    s = \"\".join([vocab_r[i] for i in tensor.tolist()])\n",
    "    ans = s[s.find('=')+1:s.find('<EOS>')].replace('<BOS>','').replace('<EOS>','')\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78dbd5b7-7dba-45a6-ad5a-c5440f4f1b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 027516696646+3336937900=\n",
      "y: 30853634546\n"
     ]
    }
   ],
   "source": [
    "x,y = get_data() \n",
    "print('x: '+''.join(x).replace('<BOS>',''))\n",
    "print('y: '+''.join(y).replace('<EOS>',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d1afe3e-30fe-4193-ad9e-c5553de1dfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 12,  4,  9,  7,  3,  8,  8, 11,  8,  8,  6,  8, 13,  5,  5,  5,  8,\n",
       "         11,  5,  9, 11, 12, 12, 14,  5, 12, 10,  7,  5,  8,  5,  6,  7,  6,  8,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, 12, 12,  2,  2,  2,\n",
       "          2,  2,  2,  2, 12,  5,  5, 12,  5,  2,  2,  2,  2,  2,  2,  8, 12, 12,\n",
       "          5,  5,  5,  9,  5, 12,  5,  5,  9,  5,  9,  5,  5,  5,  5,  9,  5,  5,\n",
       "          5, 12, 12,  5,  5,  5,  5,  5,  5,  5,  5,  8,  8,  2,  2,  2,  2]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.tensor([[vocab[i] for i in x]]) \n",
    "out = model.generate(inputs=input_ids)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "443ea928-941e-4112-b8da-0da954df9c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'30853634546'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ans(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1810f8a6-2ade-4ff8-a309-7f62188a6c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (128). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "100%|██████████| 200/200 [04:23<00:00,  1.32s/it, acc=0.925]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc= 0.925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "loop = tqdm(range(1,201))\n",
    "correct = 0\n",
    "for i in loop:\n",
    "    x,y = get_data() \n",
    "    input_ids = torch.tensor([[vocab[i] for i in x]]) \n",
    "    out = model.generate(inputs=input_ids)\n",
    "    pred = get_ans(out[0])\n",
    "    gt = ''.join(y).replace('<EOS>','')\n",
    "    if pred==gt:\n",
    "        correct+=1\n",
    "    loop.set_postfix(acc = correct/i)\n",
    "    \n",
    "print(\"acc=\",correct/len(loop))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "多训练几个epoch（epochs=50），acc可以达到99%"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "本节完。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
